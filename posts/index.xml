<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Nico Adamo</title><link>/posts/</link><description>Recent content in Posts on Nico Adamo</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&amp;copy; 2021 Nico Adamo</copyright><lastBuildDate>Sat, 18 Dec 2021 07:59:06 -0800</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Adversarial Machine Learning: An Underutilized and Dangerous Attack Surface</title><link>/posts/attacks/</link><pubDate>Sat, 18 Dec 2021 07:59:06 -0800</pubDate><guid>/posts/attacks/</guid><description>Preface Generally, when a company builds security into a product, they&amp;rsquo;re thinking about common attack vectors: buffer overflows, SQL injection, denial-of-service, and so on. These types of attacks are reasonably well-understood and there are many established practices for building defenses against them. But when incorporating ML into products, although a whole suite of solutions exist for &amp;ldquo;secure model deployment&amp;rdquo;, little thought tends to be given to the security implications of the model itself. Despite adversarial machine learning constituting it&amp;rsquo;s own proper field, I&amp;rsquo;ve found most programmers and even security researchers brush off these techniques as too academic and impractical to be worth worrying about in most real-world situations.</description></item></channel></rss>